import logging

import msgpack
from qtpy.QtCore import QTimer, QObject

from bluesky.run_engine import DocumentNames
import bluesky_kafka

from ..qt.threading import create_worker

# proposal
logger = logging.getLogger(name="bluesky.widgets.kafka")

LOADING_LATENCY = 0.01


class RemoteDispatcher(QObject):
    """
    Dispatch documents received over the network from a Kafka broker.

    This is designed to be run in a Qt application.

    Parameters
    ----------
    topics: list
        List of topics as strings such as ["topic-1", "topic-2"]
    bootstrap_servers : str
        Comma-delimited list of Kafka server addresses as a string
        such as ``'127.0.0.1:9092'``
    group_id: str
        Required string identifier for Kafka Consumer group
    consumer_config: dict
        Override default configuration or specify additional configuration
        options to confluent_kafka.Consumer.
    polling_duration: float
        Time in seconds to wait for a message before running function
        work_while_waiting. Default is 0.05.
    deserializer: function, optional
        optional function to deserialize data. Default is msgpack.loads.

    Example
    -------
    Plot data from all documents generated by remote RunEngines.
    >>> d = RemoteDispatcher(
    >>>         topics=["abc.bluesky.documents", "ghi.bluesky.documents"],
    >>>         bootstrap_servers='localhost:9092',
    >>>         group_id="document-printers",
    >>>         consumer_config={
    >>>             "auto.offset.reset": "latest"
    >>>         }
    >>>    )
    >>> d.subscribe(stream_documents_into_runs(model.add_run))
    >>> d.start()  # launches periodic workers on background threads
    >>> d.stop()  # stop launching workers
    """

    def __init__(
        self,
        topics,
        bootstrap_servers,
        group_id,
        consumer_config=None,
        polling_duration=0.05,
        deserializer=msgpack.loads,
        parent_qobject=None,
    ):
        super().__init__(parent_qobject)
        self.closed = False
        self._timer = QTimer(self)
        self._dispatcher = bluesky_kafka.RemoteDispatcher(
            topics=topics,
            bootstrap_servers=bootstrap_servers,
            group_id=group_id,
            consumer_config=consumer_config,
            polling_duration=polling_duration,
            deserializer=deserializer,
        )
        self.subscribe = self._dispatcher.subscribe
        self._waiting_for_start = True

    def _receive_data(self):
        # TODO Think about back pressure.
        while True:
            msg = self.consumer.poll(self.polling_duration)
            if msg is None:
                # no message was delivered
                # do some work before polling again
                ##work_during_wait()
                ...
            elif msg.error():
                logger.error("Kafka Consumer error: %s", msg.error())
            else:
                try:
                    name, document = self._dispatcher._bluesky_consumer._deserializer(
                        msg.value()
                    )
                    if self._waiting_for_start:
                        if name == "start":
                            self._waiting_for_start = False
                        else:
                            # We subscribed midstream and are seeing documents for
                            # which we do not have the full run. Wait for a 'start'
                            # doc.
                            return

                    yield name, document
                except Exception as exc:
                    logger.exception(exc)

    def start(self):
        if self.closed:
            raise RuntimeError(
                "This RemoteDispatcher has already been "
                "started and interrupted. Create a fresh "
                "instance with {}".format(repr(self))
            )
        self._work_loop()

    def _work_loop(self):
        worker = create_worker(
            self._receive_data,
        )
        # Schedule this method to be run again after a brief wait.
        worker.finished.connect(
            lambda: self._timer.singleShot(LOADING_LATENCY, self._work_loop)
        )
        worker.yielded.connect(self._process_result)
        worker.start()

    def _process_result(self, result):
        if result is None:
            return
        name, doc = result
        self._dispatcher.process(DocumentNames[name], doc)

    def stop(self):
        self.closed = True
